[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Emilia Morales, I was born in Quito-Ecuador and now I live in Stockholm-Sweden. I do bioinformatics in translational breast cancer research, where we study how different treatments affect tumors with the aim of developing personalized treatment approaches."
  },
  {
    "objectID": "posts/Day 1/index.html",
    "href": "posts/Day 1/index.html",
    "title": "Day 1: Data Management, Reproducible Research and How to make a Quarto blog",
    "section": "",
    "text": "Information to remember\n\nWe work with data, so it’s important to remember the FAIR principles about good data management practices:\n\n\n\n\nFAIR principles\n\n\n\nData management things to take into account:\n\nThe raw data should be in a folder alone where it won’t get altered, and separated from the code and results.\nThere should be information about what I did in every project folder, for me to remember later and also for others to understand if the code is to be passed on, preferably with a README file. It’s also advisable to write code in scripts combined with text for documentation and explanations (literate programming).\nName files properly without special characters and spaces.\n\nI like to start with the date (YY.MM.DD_XX.X).\n\nUse version control (git-github) to track all the changes made to the files.\nUse environment managers and/or containers to manage packages and maintain the same versions across systems and time.\nSimplify workflows by standardizing the steps with a workflow manager or establish a new proper pipeline.\n\n\n\nNote: This material was taken from the course’s webpage.\n\n\n\n\nWhat we did today\nToday was the first day of the Applied Bioinformatics course. We learned in general about good practices in reproducible research, and we had an exercise with GitHub. We each created a branch with our name, and added our name to the README file. Then we merged this branch with someone else’s, resolving the emerging conflicts on. Then we merged this branch with someone else and so on, after multiple merges we created a student list with all of us in alphabetical order. It was challenging to go through with all the commands of pushing, pulling, committing, adding etc., but the more I practice, the easier it gets and the better it flows.\nThen, we created a blog in VSCode which is published in Github pages with Quarto; this is where this entry is being written. This is a very useful tool, as it basically allows us to create a webpage in a simple way, which is backed up by Github. It’s again a good way to practice the commands related with version control in Github, and there’s a lot of applications to this. In this blog we will record and report everything we do throughout this course.\nToday I’m testing different functionalities and things I can edit in the webpage when creating blog posts.\n\nThe opening picture shows my dog, her name is Carmela and she’s very cute"
  },
  {
    "objectID": "posts/Day 4/index.html",
    "href": "posts/Day 4/index.html",
    "title": "Day 4: RNA-Seq data analysis with Nextflow and nf-core pipelines",
    "section": "",
    "text": "Today we could run correctly the RNA-Seq pipeline with nextflow and nf-core. First, we tested the workflow manager nextflow by chaining the steps for qc and multiqc. Then we tested the ‘rnaseq’ pipeline by nf-core, which was built with nextflow. We used some of the RNA samples we had previously received, and ran the pipeline."
  },
  {
    "objectID": "posts/Day 2/index.html",
    "href": "posts/Day 2/index.html",
    "title": "Day 2: Environments with Pixi, SLURM, containers and QC exercises",
    "section": "",
    "text": "Information to remember\nBioinformatics environments\n\nWe can create a virtual environment to define and manage different tools and their dependencies we need for our analyses, in order to keep the workflow reproducible.\nWith environments, we can run the same analysis in different computers with different operating systems.\nIt’s easier to create this than installing and loading every package in the specific version every time we want to reproduce some old result.\nExamples of environment managers are conda (bioconda for more biology related stuff) and Pixi, which we will use today.\nPixi runs on all major operating systems, and allows to include multiple platforms within a single environment.\nIt’s good to create an environment for every project.\n\nContainers\n\nAnother system that can help greatly with assuring reproducibility in our bioinformatics analyses is the use of containers. A container is a software that’s built with ‘images’ of all its content.\nThey can be run on any computer and system, they contain everything needed to run an application.\nThey include, apart from the packages and dependencies, the operating system.\nThey require a container management tool to run a fully isolated system on top of the computer’s OS.\nThe different with environment managers is that environments manage tools within our system, while containers package the entire system the tools run in.\nExamples: Docker, Apptainer, Podman.\nThere are several public container images with established purposes. Common sites to download them are Dockerhub https://hub.docker.com/ (Docker) and Seqera https://seqera.io/containers/ (Docker or Singularity/Apptainer). Seqera also allows you to ‘make’ container images as you wish.\n\n\nNote: This material was taken from the course’s webpage.\n\n\n\n\nWhat we did today\nGetting started with Pixi\nOn the second day of the course, we performed quality control exercises in RNA sequencing samples with different methods. First, we worked with Pixi, an environment manager. We logged into the course server and created a Pixi environment, where we added the conda-forge and bioconda channels:\npixi init -c conda-forge -c bioconda\nAfter running this, a new file named pixi.toml is created, where we can see all the packages loaded and their versions. And the pixi.lock file contains further information about where the packages where installed from, license information, etc. These shouldn’t be deleted or the environment will break.\nThen, we added the dependencies we wanted, like quarto, with the command pixi add quarto. In order to run the tool we installed, we can run pixi run quarto, or “enter” the Pixi environment with pixi shell and then we can avoid the pixi run command every time.\nQC exercise with Pixi\nFor the quality control exercise we practiced the use of screens, slurm and the environment in Pixi previously created. We did the quality control assessment with a tool called FastQC, which requires as input data in fastq, SAM or BAM formats and generates a .hyml report with a summary of the analysis for each sample in different aspects. We started by opening a screen. Opening a screen helps us run a process that may be longer without overloading our main terminal window, especially if we are working locally (then we can close the computer while the process is still running). To open a new screen:\nscreen -S name\nThen, in my QC directory, I created sym links to the place where the actual sequencing fastq files were stored:\nln -s path/to/common_data/RNAseq/*fastq.gz .\nWe added then the fastqc tool to our pixi environment. An in order to run the fastqc tool obtained with Pixi, we used SLURM, the job scheduling system. This way we can use the resources of a specific project instead of the login node, which doesn’t have a big computational power and many people use it at the same time. To run the tool we can use the following command:\npixi run srun -A hpc2n2025-203 -t 15:00 -n 1 fastqc --noextract -o fastqc data/sample_1.fastq.gz data/sample_2.fastq.gz \nwhere we first specify that the fastqc tool is within our Pixi environment, -A project_id, -t maximum running time, -n number_cores, ‘fastqc’ is our tool and the parameters that come afterwards are specific for it, like -o output directory and the samples (both directions) to be analyzed.\nAlternatively we can write a script that runs the tool automatically in the background instead of occupying the screen or terminal and then run it with sbatch. These scripts have the following structure:\n#! /bin/bash -l\n#SBATCH -A hpc2n2025-203\n#SBATCH -t 30:00\n#SBATCH -n 1\n\npixi run fastqc -o ../fastqc --noextract ../data/*fastq.gz\nAnd I can run it with sbatch 'script'. After running fastqc for all samples, we ran multiqc multiqc 'fastqc_directory' 'output_directory', which summarises the output of the analysis of all the samples creating one single htlm report.\nQC exercise with containers\nWe also tested the QC analysis using a container downloaded from Seqera with the fastqc tool instead of with Pixi. We ran this script with sbatch:\n#! /bin/bash -l\n#SBATCH -A hpc2n2025-203\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec -B ../data:/data ../../containers/fastqc_0.12.1--104d26ddd9519960.sif* fastqc -o ../fastqc_container --noextract ../data/*fastq.gz\nwhere the container had been previously downloaded into my containers folder. Then apptainer is called to execute the container which contains the fastqc tool, and we provide the parameters for it as before.\nBuilding a container\nIf the software we want is not in a published container, we can build one as we want. Here we built a container with a cow that told us the date or a fortune phrase, as shown in the post’s picture. Our script lolcow.def that says a fortune phrase looks like this:\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat    \nThen, to build the container we run apptainer build lolcow.sif lolcow.def and run it with apptainer run lolcow.sif. If we run instead apptainer exec lolcow.sif bash -c \"date|cowsay, we get a date because with ‘exec’, the command we specify outside when running the container overrides the one inside. The cow speaks the truth sometimes."
  },
  {
    "objectID": "posts/Day 5/index.html",
    "href": "posts/Day 5/index.html",
    "title": "Day 5: ggplot2 and wrapup",
    "section": "",
    "text": "On the final day of the course, we had a session about the R package ggplot2, which is widely used for making graphs as there are a lot of options to personalize. After learning some theory behind it, the structure and some examples, we did some exercises ourselves by following a tutorial from NBIS. With this, we had a more basic exercise, and two more complex ones where we tried to recreate some public plots."
  },
  {
    "objectID": "posts/Day 3/index.html",
    "href": "posts/Day 3/index.html",
    "title": "Day 3: Nextflow, nf-core and AI in bioinformatics",
    "section": "",
    "text": "Information to remember\nWorkflow managers\n\nWorkflow managers, such as Nextflow and Snakemake, help us simplify several steps of a pipeline by ‘wrapping’ them up and running them automatically.\nThis way, we only have to launch the pipeline with one command and the proper parameters, and it runs all the necessary steps until the result we wish. It avoids us having one script for every step we want to run.\nThey can run containers to eliminate package installation conflicts.\nThey make the processes more reproducible.\nIn Nextflow, we assign as channel our information, such as data, input, output, etc. Each actual script is called process and can be written in any language. We assign our workflow scope by defining the order of the processes and the interaction with eachother.\n\n\n\n\nNextflow in a nutshell\n\n\n\nNextflow is a Domain Specific Language (DSL) implemented as an extension of the Groovy programming language. This means that Nextflow can run any Groovy and Java code.\nIf the pipeline fails at some point, the execution can be resumed easily from the spot since results are cached in the work folder. Once we’re done running the pipeline we should delete this, as can take up a lot of disk space.\nProcesses run independently from each other. So processes that are set to run simultaneously will run in no particular order.\nUsually, the content of a channel is consumed only once.\nIn a Nextflow script, first we need to define our parameters (data) and channels (actions). Then the codes for each process. And finally the workflow scope, where we indicate what we want to be done with our parameters and channels. (The order doesn’t have to be like this strictly.)\n\nnf-core\n\nnf-core is a community built around Nextflow. There we can find several bioinformatics pipelines developed by volunteers with Nextflow.\nnf-core also provides processes as modules, if the whole pipeline is not required.\nWe can access their pipelines here. And the modules here.\nIn order to run an nf-core pipeline, we have to go its website and click on Launch version X, where we will be taken to another page to enter parameters about our samples and process, and we will get the configuration file for our run..\n\n\nNote: This material was taken from the course’s webpage.\n\n\n\n\nWhat we did today\nOn the third day of the course, we tested Netflow and nf-core. For Nextflow, first, we copied the training material from the course. Then we run a script that did some basic commands, like breaking our input text into chunks of 6 characters, and capitalizing the letters or turning them backwards. For this, we learnt how the structure of a nextflow file should be and how to manipulate each part. Then we ran the script using Pixi also. We learnt what each nextflow output file shows and why. Finally, we learnt how to clean up the working directory Nextflow generates when running our commands. Then, we tested an RNA sequencing analysis pipeline that is established in Nextflow. In the nextflow.config script we can set different parameters to be used in the further scripts, like for it to be executed with slurm, the default time limit for all processes and the project account. We then tried to run the first script that creates a transcriptome index file, using the tool salmon as a container. We couldn’t finish running this, as our cluster ran out of resources allocated for us. We learnt the theory behind nf-core, but couldn’t test it either because of the resources.\nAfterwards, we had a discussion about the use of AI in bioinformatics. It was interesting to learn that most Swedish universities allow the use of AI in official students’ works as long as it is disclosed. I think this is fair, as AI is widely integrated in our daily lives by now, and it’s not going anywhere. So we should learn to use it properly to our advantage instead of banning it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bioinformatics course - blog by Emilia",
    "section": "",
    "text": "Day 5: ggplot2 and wrapup\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 10, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4: RNA-Seq data analysis with Nextflow and nf-core pipelines\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 9, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3: Nextflow, nf-core and AI in bioinformatics\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 8, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2: Environments with Pixi, SLURM, containers and QC exercises\n\n\n\npost\n\n\n\nTools that help with analysis reproducibility\n\n\n\n\n\nOct 7, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1: Data Management, Reproducible Research and How to make a Quarto blog\n\n\n\npost\n\n\n\nThis blog is created with Quarto, and published with GitHub pages\n\n\n\n\n\nOct 6, 2025\n\n\nEmilia Morales\n\n\n\n\n\nNo matching items"
  }
]