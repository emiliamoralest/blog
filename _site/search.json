[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Emilia Morales, I was born in Quito-Ecuador and now I live in Stockholm-Sweden. I do bioinformatics in translational breast cancer research, where we study how different treatments affect tumors with the aim of developing personalized treatment approaches."
  },
  {
    "objectID": "posts/Day 1/index.html",
    "href": "posts/Day 1/index.html",
    "title": "Day 1: Data Management, Reproducible Research and How to make a Quarto blog",
    "section": "",
    "text": "Information to remember\n\nWe work with data, so it’s important to remember the FAIR principles about good data management practices:\n\n\n\n\nFAIR principles\n\n\n\nData management things to take into account:\n\nThe raw data should be in a folder alone where it won’t get altered, and separated from the code and results.\nThere should be information about what I did in every project folder, for me to remember later and also for others to understand if the code is to be passed on, preferably with a README file. It’s also advisable to write code in scripts combined with text for documentation and explanations (literate programming).\nName files properly without special characters and spaces.\n\nI like to start with the date (YY.MM.DD_XX.X).\n\nUse version control (git-github) to track all the changes made to the files.\nUse environment managers and/or containers to manage packages and maintain the same versions across systems and time.\nSimplify workflows by standardizing the steps with a workflow manager or establish a new proper pipeline.\n\n\n\nNote: This material was taken from the course’s webpage.\n\n\n\n\nWhat we did today\nToday was the first day of the Applied Bioinformatics course. We learned in general about good practices in reproducible research, and we had an exercise with GitHub. We each created a branch with our name, and added our name to the README file. Then we merged this branch with someone else’s, resolving the emerging conflicts on. Then we merged this branch with someone else and so on, after multiple merges we created a student list with all of us in alphabetical order. It was challenging to go through with all the commands of pushing, pulling, committing, adding etc., but the more I practice, the easier it gets and the better it flows.\nThen, we created a blog in VSCode which is published in Github pages with Quarto; this is where this entry is being written. This is a very useful tool, as it basically allows us to create a webpage in a simple way, which is backed up by Github. It’s again a good way to practice the commands related with version control in Github, and there’s a lot of applications to this. In this blog we will record and report everything we do throughout this course. Useful links with personalization options for Quarto:\n\nhttps://quarto.org/docs/reference/formats/html.html\nhttps://quarto.org/docs/output-formats/html-themes.html\nhttps://quarto.org/docs/output-formats/html-code.html\n\nImportant commands for publishing the blog:\n\nquarto preview\nquarto render\nquarto publish gh-pages\ngit add –all && git commit -m ” ” && git push origin main\n\nToday I’m testing different functionalities and things I can edit in the webpage when creating blog posts.\n\nThe opening picture shows my dog, her name is Carmela and she’s very cute"
  },
  {
    "objectID": "posts/Day 4/index.html",
    "href": "posts/Day 4/index.html",
    "title": "Day 4: RNA-Seq data analysis with Nextflow and nf-core pipelines",
    "section": "",
    "text": "What we did today\nToday we could run correctly the RNA-Seq pipeline with Nextflow and nf-core, as a continuation from yesterday.\nNextflow RNA-Seq pipeline\nWe want to create a Nextflow pipeline for RNA-Seq data, which indexes a transcriptome fi, does QC, performs quantification and creates a multiqc report. For this, we will create a Pixi environment and add the tools we need, starting with nextflow, salmon, fastqc. Although if we use the containers (recommended), we don’t need to add them to Pixi, only nextflow must be there. For our pipeline, we need to establish that the last step, multiqc, needs to wait for the results of the quantification and fastqc steps. This is how our script script.nf looked at the end:\n#!/usr/bin/env nextflow\n\n/*\n * pipeline input parameters\n */\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n\nlog.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n    .stripIndent()\n\n/*\n * define the `INDEX` process that creates a binary index\n * given the transcriptome file\n */\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path 'salmon_index'\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    container 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n    publishDir \"$params.outdir/salmon_quantification\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}\"\n\n    container 'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0'\n    publishDir \"$params.outdir/fastqc\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}\n    fastqc --noextract -o fastqc_${sample_id} ${reads[0]} ${reads[1]}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    input:\n    path '*'\n\n    output:\n    path \"multiqc_report.html\"\n\n    container 'community.wave.seqera.io/library/multiqc:1.31--1efbafd542a23882'\n    publishDir \"$params.outdir/multiqc\"\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}\n\nworkflow {\n    Channel\n        .fromFilePairs(params.reads, checkIfExists: true)\n        .set { read_pairs_ch }\n\n    index_ch = INDEX(params.transcriptome_file)\n    quant_ch = QUANTIFICATION(index_ch, read_pairs_ch)\n    fastqc_ch = FASTQC(read_pairs_ch)\n    MULTIQC(quant_ch.mix(fastqc_ch).collect())\n}\nAnd our nextflow.config file looked like this:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n\n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\n\n    withName:'QUANTIFICATION'{\n        time = 10.m\n        cpus = 2\n    }\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = 'hpc2n2025-203'\n}\nnf-core RNA-Seq pipeline\nThen we tested the rnaseq pipeline by nf-core, which was built with Nextflow. We used some of the RNA samples we had previously received, and ran the pipeline. We started by addind nextflow and nf-core to our Pixi environment. Then we went to the nf-core launcher in the website and added our data: absolute path of the dir of the samplesheet.csv file we created according to the instructions with input data informayion, and absolute path of the output dir. Then, we downloaded the needed reference genome files (fasta and gtf) and provided those paths to the launcher. Our resulting nf-params.json file looked like this:\n{\n    \"input\": \"\\/proj\\/nobackup\\/medbioinfo2025\\/emilia_morales\\/nf-core-rnaseq\\/samplesheet.csv\",\n    \"outdir\": \".\\/results\",\n    \"fasta\": \"\\/proj\\/nobackup\\/medbioinfo2025\\/emilia_morales\\/nf-core-rnaseq\\/reference\\/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa\",\n    \"gtf\": \"\\/proj\\/nobackup\\/medbioinfo2025\\/emilia_morales\\/nf-core-rnaseq\\/reference\\/Homo_sapiens.GRCh38.108.gtf\"\n}\nWe also prepared the hpc2n.config file:\n// Config profile for HPC2N\nparams {\n  config_profile_description = 'Cluster profile for HPC2N'\n  config_profile_contact = 'Pedro Ojeda @pojeda'\n  config_profile_url = 'https://www.hpc2n.umu.se/'\n  project = 'hpc2n2025-203'\n  clusterOptions = null\n  max_memory = 128.GB\n  max_cpus = 28\n  max_time = 168.h\n  email = 'emilia.morales@ki.se'\n}\n\nsingularity {\n  enabled = true\n}\n\nprocess {\n  executor = 'slurm'\n  clusterOptions = { \"-A $params.project ${params.clusterOptions ?: ''}\" }\n}\nThe final command to run the whole pipeline is:\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\nAfter we are done running what we want, we can see the previous runs with pixi run nextflow log, and then clean up our working directory with pixi run nextflow clean -f -before &lt;run_name&gt; to keep the log of a run, or simply pixi run nextflow clean -f to clean everything."
  },
  {
    "objectID": "posts/Day 2/index.html",
    "href": "posts/Day 2/index.html",
    "title": "Day 2: Environments with Pixi, SLURM, containers and QC exercises",
    "section": "",
    "text": "Information to remember\nBioinformatics environments\n\nWe can create a virtual environment to define and manage different tools and their dependencies we need for our analyses, in order to keep the workflow reproducible.\nWith environments, we can run the same analysis in different computers with different operating systems.\nIt’s easier to create this than installing and loading every package in the specific version every time we want to reproduce some old result.\nExamples of environment managers are conda (bioconda for more biology related stuff) and Pixi, which we will use today.\nPixi runs on all major operating systems, and allows to include multiple platforms within a single environment.\nIt’s good to create an environment for every project.\n\nContainers\n\nAnother system that can help greatly with assuring reproducibility in our bioinformatics analyses is the use of containers. A container is a software that’s built with ‘images’ of all its content.\nThey can be run on any computer and system, they contain everything needed to run an application.\nThey include, apart from the packages and dependencies, the operating system.\nThey require a container management tool to run a fully isolated system on top of the computer’s OS.\nThe different with environment managers is that environments manage tools within our system, while containers package the entire system the tools run in.\nExamples: Docker, Apptainer, Podman.\nThere are several public container images with established purposes. Common sites to download them are Dockerhub https://hub.docker.com/ (Docker) and Seqera https://seqera.io/containers/ (Docker or Singularity/Apptainer). Seqera also allows you to ‘make’ container images as you wish.\n\n\nNote: This material was taken from the course’s webpage.\n\n\n\n\nWhat we did today\nGetting started with Pixi\nOn the second day of the course, we performed quality control exercises in RNA sequencing samples with different methods. First, we worked with Pixi, an environment manager. We logged into the course server and created a Pixi environment, where we added the conda-forge and bioconda channels:\npixi init -c conda-forge -c bioconda\nAfter running this, a new file named pixi.toml is created, where we can see all the packages loaded and their versions. And the pixi.lock file contains further information about where the packages where installed from, license information, etc. These shouldn’t be deleted or the environment will break.\nThen, we added the dependencies we wanted, like quarto, with the command pixi add quarto. In order to run the tool we installed, we can run pixi run quarto, or “enter” the Pixi environment with pixi shell and then we can avoid the pixi run command every time.\nQC exercise with Pixi\nFor the quality control exercise we practiced the use of screens, slurm and the environment in Pixi previously created. We did the quality control assessment with a tool called FastQC, which requires as input data in fastq, SAM or BAM formats and generates a .hyml report with a summary of the analysis for each sample in different aspects. We started by opening a screen. Opening a screen helps us run a process that may be longer without overloading our main terminal window, especially if we are working locally (then we can close the computer while the process is still running). To open a new screen:\nscreen -S name\nThen, in my QC directory, I created sym links to the place where the actual sequencing fastq files were stored:\nln -s path/to/common_data/RNAseq/*fastq.gz .\nWe added then the fastqc tool to our pixi environment. An in order to run the fastqc tool obtained with Pixi, we used SLURM, the job scheduling system. This way we can use the resources of a specific project instead of the login node, which doesn’t have a big computational power and many people use it at the same time. To run the tool we can use the following command:\npixi run srun -A hpc2n2025-203 -t 15:00 -n 1 fastqc --noextract -o fastqc data/sample_1.fastq.gz data/sample_2.fastq.gz \nwhere we first specify that the fastqc tool is within our Pixi environment, -A project_id, -t maximum running time, -n number_cores, ‘fastqc’ is our tool and the parameters that come afterwards are specific for it, like -o output directory and the samples (both directions) to be analyzed.\nAlternatively we can write a script that runs the tool automatically in the background instead of occupying the screen or terminal and then run it with sbatch. These scripts have the following structure:\n#! /bin/bash -l\n#SBATCH -A hpc2n2025-203\n#SBATCH -t 30:00\n#SBATCH -n 1\n\npixi run fastqc -o ../fastqc --noextract ../data/*fastq.gz\nAnd I can run it with sbatch 'script'. After running fastqc for all samples, we ran multiqc multiqc 'fastqc_directory' 'output_directory', which summarises the output of the analysis of all the samples creating one single htlm report.\nQC exercise with containers\nWe also tested the QC analysis using a container downloaded from Seqera with the fastqc tool instead of with Pixi. We ran this script with sbatch:\n#! /bin/bash -l\n#SBATCH -A hpc2n2025-203\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec -B ../data:/data ../../containers/fastqc_0.12.1--104d26ddd9519960.sif* fastqc -o ../fastqc_container --noextract ../data/*fastq.gz\nwhere the container had been previously downloaded into my containers folder. Then apptainer is called to execute the container which contains the fastqc tool, and we provide the parameters for it as before.\nBuilding a container\nIf the software we want is not in a published container, we can build one as we want. Here we built a container with a cow that told us the date or a fortune phrase, as shown in the post’s picture. Our script lolcow.def that says a fortune phrase looks like this:\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat    \nThen, to build the container we run apptainer build lolcow.sif lolcow.def and run it with apptainer run lolcow.sif. If we run instead apptainer exec lolcow.sif bash -c \"date|cowsay, we get a date because with ‘exec’, the command we specify outside when running the container overrides the one inside. The cow speaks the truth sometimes."
  },
  {
    "objectID": "posts/Day 5/index.html",
    "href": "posts/Day 5/index.html",
    "title": "Day 5: ggplot2 and wrap-up",
    "section": "",
    "text": "On the final day of the course, we had a session about the R package ggplot2, which is widely used for making graphs in R as there are a lot of options to personalize. Some useful links provided in the presentation:\n\nggplot2 extensions https://exts.ggplot2.tidyverse.org/\nA curated list of awesome ggplot2 tutorials, packages etc. https://github.com/erikgahner/awesome-ggplot2\nOfficial cheatsheet https://ggplot2.tidyverse.org/reference/\nRStudio cheatsheets, including Quarto and several useful packages https://posit.co/resources/cheatsheets/\n\nAfter learning some theory behind ggplot2, the structure and some examples, we did some exercises ourselves by following a tutorial from NBIS. With this, we had a more basic exercise, and two more complex ones where we tried to recreate some public plots. Here are the resulting plots made by me:\n\n\n\nScatterplot\n\n\n\n\n\nHeatmap"
  },
  {
    "objectID": "posts/Day 3/index.html",
    "href": "posts/Day 3/index.html",
    "title": "Day 3: Nextflow, nf-core and AI in bioinformatics",
    "section": "",
    "text": "Information to remember\nWorkflow managers\n\nWorkflow managers, such as Nextflow and Snakemake, help us simplify several steps of a pipeline by ‘wrapping’ them up and running them automatically.\nThis way, we only have to launch the pipeline with one command and the proper parameters, and it runs all the necessary steps until the result we wish. It avoids us having one script for every step we want to run.\nThey can run containers to eliminate package installation conflicts.\nThey make the processes more reproducible.\nIn Nextflow, we assign as channel our information, such as data, input, output, etc. Each actual script is called process and can be written in any language. We assign our workflow scope by defining the order of the processes and the interaction with eachother.\n\n\n\n\nNextflow in a nutshell\n\n\n\nNextflow is a Domain Specific Language (DSL) implemented as an extension of the Groovy programming language. This means that Nextflow can run any Groovy and Java code.\nIf the pipeline fails at some point, the execution can be resumed easily from the spot since results are cached in the work folder. Once we’re done running the pipeline we should delete this, as can take up a lot of disk space.\nProcesses run independently from each other. So processes that are set to run simultaneously will run in no particular order.\nUsually, the content of a channel is consumed only once.\nIn a Nextflow script, first we need to define our parameters (data) and channels (actions). Then the codes for each process. And finally the workflow scope, where we indicate what we want to be done with our parameters and channels. (The order doesn’t have to be like this strictly.)\nWe also need a nextflow.config file where we can choose slurm as the executor, how many cpus to use and the maximum run time. We can also set individual choices for each process.\n\nnf-core\n\nnf-core is a community built around Nextflow. There we can find several bioinformatics pipelines developed by volunteers with Nextflow.\nnf-core also provides processes as modules, if the whole pipeline is not required.\nWe can access their pipelines here. And the modules here.\nIn order to run an nf-core pipeline, we have to go its website and click on Launch version X, where we will be taken to another page to enter parameters about our samples and process, and we will get a JSON file necessary for our run.\nWe will also need an hpc2n.config file with our project ID information.\nImportant: nf-core configs ready for UPPMAX https://nf-co.re/configs/uppmax/\n\n\nNote: This material was taken from the course’s webpage.\n\n\n\n\nWhat we did today\nOn the third day of the course, we tested Netflow and nf-core. For Nextflow, first, we copied the training material from the course. Then we run a script that did some basic commands, like breaking our input text into chunks of 6 characters, and capitalizing the letters or turning them backwards. For this, we learnt how the structure of a nextflow file should be and how to manipulate each part. Then we ran the script using Pixi also. We learnt what each nextflow output file shows and why. Finally, we learnt how to clean up the working directory Nextflow generates when running our commands. Then, we tested an RNA sequencing analysis pipeline that is established in Nextflow. In the nextflow.config script we can set different parameters to be used in the further scripts, like for it to be executed with slurm, the default time limit for all processes and the project account. We then tried to run the first script that creates a transcriptome index file, using the tool salmon as a container. We couldn’t finish running this, as our cluster ran out of resources allocated for us. We learnt the theory behind nf-core, but couldn’t test it either because of the resources.\nAfterwards, we had a discussion about the use of AI in bioinformatics. It was interesting to learn that most Swedish universities allow the use of AI in official students’ works as long as it is disclosed. I think this is fair, as AI is widely integrated in our daily lives by now, and it’s not going anywhere. So we should learn to use it properly to our advantage instead of banning it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bioinformatics course - blog by Emilia",
    "section": "",
    "text": "Day 5: ggplot2 and wrap-up\n\n\n\npost\n\n\n\nIntroduction to ggplot2 and course final remarks\n\n\n\n\n\nOct 10, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4: RNA-Seq data analysis with Nextflow and nf-core pipelines\n\n\n\npost\n\n\n\nExercises with Nextflow and nf-core\n\n\n\n\n\nOct 9, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3: Nextflow, nf-core and AI in bioinformatics\n\n\n\npost\n\n\n\nIntroduction to workflow managers and AI talks\n\n\n\n\n\nOct 8, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2: Environments with Pixi, SLURM, containers and QC exercises\n\n\n\npost\n\n\n\nTools that help with analysis reproducibility\n\n\n\n\n\nOct 7, 2025\n\n\nEmilia Morales\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1: Data Management, Reproducible Research and How to make a Quarto blog\n\n\n\npost\n\n\n\nThis blog is created with Quarto, and published with GitHub pages\n\n\n\n\n\nOct 6, 2025\n\n\nEmilia Morales\n\n\n\n\n\nNo matching items"
  }
]